{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1610933,"sourceType":"datasetVersion","datasetId":935180},{"sourceId":1622428,"sourceType":"datasetVersion","datasetId":958864},{"sourceId":1624602,"sourceType":"datasetVersion","datasetId":960259},{"sourceId":10091429,"sourceType":"datasetVersion","datasetId":6216528}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"NEMO = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:12:39.659799Z","iopub.execute_input":"2024-12-08T13:12:39.660049Z","iopub.status.idle":"2024-12-08T13:12:39.667335Z","shell.execute_reply.started":"2024-12-08T13:12:39.660022Z","shell.execute_reply":"2024-12-08T13:12:39.666499Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:12:39.674574Z","iopub.execute_input":"2024-12-08T13:12:39.674815Z","iopub.status.idle":"2024-12-08T13:13:24.010366Z","shell.execute_reply.started":"2024-12-08T13:12:39.674790Z","shell.execute_reply":"2024-12-08T13:13:24.009608Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"if NEMO:\n    !apt-get update && apt-get install -y libsndfile1 ffmpeg\n    !pip install Cython packaging\n    !pip install nemo_toolkit['tts']\n\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install setuptools==69.5.1\n!pip install causal-conv1d && pip install mamba-ssm==2.2.2  # only GPU\n\n!pip install speechbrain\n!pip install -U encodec\n\n!pip install -q parallel_wavegan PyYaml unidecode ConfigArgparse g2p_en espnet_tts_frontend\n!pip install --upgrade --no-cache-dir gdown\n!git clone -q https://github.com/espnet/espnet.git\n!cd espnet && git fetch && git checkout -b v.0.9.1 refs/tags/v.0.9.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:13:24.012053Z","iopub.execute_input":"2024-12-08T13:13:24.012933Z","iopub.status.idle":"2024-12-08T13:16:51.291797Z","shell.execute_reply.started":"2024-12-08T13:13:24.012891Z","shell.execute_reply":"2024-12-08T13:16:51.290668Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nCollecting setuptools==69.5.1\n  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\nDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: setuptools\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 70.0.0\n    Uninstalling setuptools-70.0.0:\n      Successfully uninstalled setuptools-70.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nconda 24.9.0 requires packaging>=23.0, but you have packaging 21.3 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed setuptools-69.5.1\nCollecting causal-conv1d\n  Downloading causal_conv1d-1.5.0.post8.tar.gz (9.4 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from causal-conv1d) (2.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from causal-conv1d) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from causal-conv1d) (1.11.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->causal-conv1d) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->causal-conv1d) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->causal-conv1d) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->causal-conv1d) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->causal-conv1d) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->causal-conv1d) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->causal-conv1d) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->causal-conv1d) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->causal-conv1d) (1.3.0)\nBuilding wheels for collected packages: causal-conv1d\n  Building wheel for causal-conv1d (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for causal-conv1d: filename=causal_conv1d-1.5.0.post8-cp310-cp310-linux_x86_64.whl size=103962594 sha256=3d32e60e5d00dc067c80e6bb753862f024c7706639e80175e5b47a332d5fb6ed\n  Stored in directory: /root/.cache/pip/wheels/75/ef/0a/d9abf869acdd5fc07f403f4d8dd9db650cd66e81528a907941\nSuccessfully built causal-conv1d\nInstalling collected packages: causal-conv1d\nSuccessfully installed causal-conv1d-1.5.0.post8\nCollecting mamba-ssm==2.2.2\n  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from mamba-ssm==2.2.2) (2.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mamba-ssm==2.2.2) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from mamba-ssm==2.2.2) (1.11.1.1)\nCollecting einops (from mamba-ssm==2.2.2)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting triton (from mamba-ssm==2.2.2)\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from mamba-ssm==2.2.2) (4.45.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->mamba-ssm==2.2.2) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm==2.2.2) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm==2.2.2) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm==2.2.2) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm==2.2.2) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm==2.2.2) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->mamba-ssm==2.2.2) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm==2.2.2) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm==2.2.2) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm==2.2.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm==2.2.2) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm==2.2.2) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm==2.2.2) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm==2.2.2) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->mamba-ssm==2.2.2) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->mamba-ssm==2.2.2) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm==2.2.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm==2.2.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm==2.2.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->mamba-ssm==2.2.2) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->mamba-ssm==2.2.2) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: mamba-ssm\n  Building wheel for mamba-ssm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=323998290 sha256=a658a5438dbe9fb3a53799d7d9f4714ca09225769c24ec04a403728ac7d1c69e\n  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\nSuccessfully built mamba-ssm\nInstalling collected packages: triton, einops, mamba-ssm\nSuccessfully installed einops-0.8.0 mamba-ssm-2.2.2 triton-3.1.0\nCollecting speechbrain\n  Downloading speechbrain-1.0.2-py3-none-any.whl.metadata (23 kB)\nCollecting hyperpyyaml (from speechbrain)\n  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from speechbrain) (1.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from speechbrain) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from speechbrain) (21.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from speechbrain) (1.14.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from speechbrain) (0.2.0)\nRequirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from speechbrain) (2.4.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (from speechbrain) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from speechbrain) (4.66.4)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from speechbrain) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (2024.6.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->speechbrain) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->speechbrain) (2.32.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->speechbrain) (3.1.2)\nRequirement already satisfied: ruamel.yaml>=0.17.28 in /opt/conda/lib/python3.10/site-packages (from hyperpyyaml->speechbrain) (0.18.6)\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.10/site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.8)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->speechbrain) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->speechbrain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->speechbrain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->speechbrain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->speechbrain) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\nDownloading speechbrain-1.0.2-py3-none-any.whl (824 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m824.8/824.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\nInstalling collected packages: hyperpyyaml, speechbrain\nSuccessfully installed hyperpyyaml-1.2.2 speechbrain-1.0.2\nCollecting encodec\n  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from encodec) (1.26.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from encodec) (2.4.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (from encodec) (2.4.0)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from encodec) (0.8.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->encodec) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->encodec) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->encodec) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->encodec) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->encodec) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->encodec) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->encodec) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->encodec) (1.3.0)\nBuilding wheels for collected packages: encodec\n  Building wheel for encodec (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45762 sha256=0d0f183c881629d848a198db282393979ff1ba503ad16f901c4acf5edf26ca8d\n  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\nSuccessfully built encodec\nInstalling collected packages: encodec\nSuccessfully installed encodec-0.1.1\nRequirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nSwitched to a new branch 'v.0.9.1'\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport string\nimport time\nimport shutil\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.profiler import record_function\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\n\nimport torchaudio\nimport librosa\nfrom librosa.feature import melspectrogram\nimport soundfile as sf\n\nfrom matplotlib import pyplot as plt\nimport IPython.display as ipd\nfrom tqdm import tqdm, notebook\n\nfrom encodec import EncodecModel\nfrom encodec.utils import convert_audio\n\nfrom einops import rearrange, repeat\nfrom mamba_ssm import Mamba\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\nfrom g2p_en import G2p\nimport nltk\nfrom speechbrain.inference.speaker import EncoderClassifier\nfrom accelerate import Accelerator\n\n\nif NEMO:\n    from nemo_text_processing.text_normalization.normalize import Normalizer\n    from nemo.collections.tts.torch.tts_tokenizers import EnglishPhonemesTokenizer\n    from nemo.collections.tts.g2p.models.en_us_arpabet import EnglishG2p\n    from nemo.collections.tts.models import AlignerModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nTRAIN_DATA_PATH = '/kaggle/input/libri-tts-train-clean-100/train-clean-100'\nTEST_DATA_PATH = '/kaggle/input/libri-tts-test/test-clean/test-clean'\nVAL_DATA_PATH = '/kaggle/input/libri-tts-dev/dev-clean/dev-clean'\nARPABET = '/kaggle/input/arpabet/ARPABET.txt'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:16:51.293718Z","iopub.execute_input":"2024-12-08T13:16:51.294609Z","iopub.status.idle":"2024-12-08T13:17:09.811815Z","shell.execute_reply.started":"2024-12-08T13:16:51.294561Z","shell.execute_reply":"2024-12-08T13:17:09.810981Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout):\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, grad_output):\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n/opt/conda/lib/python3.10/site-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout, *args):\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package cmudict to /usr/share/nltk_data...\n[nltk_data]   Package cmudict is already up-to-date!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"batch, length, dim = 2, 64, 16\nx = torch.randn(batch, length, dim).to(\"cuda\")\nmamba_ = Mamba(\n    # This module uses roughly 3 * expand * d_model^2 parameters\n    d_model=dim, # Model dimension d_model\n    d_state=16,  # SSM state expansion factor\n    d_conv=4,    # Local convolution width\n    expand=2,    # Block expansion factor\n).to(\"cuda\")\ny = mamba_(x)\nassert y.shape == x.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:17:09.814246Z","iopub.execute_input":"2024-12-08T13:17:09.814846Z","iopub.status.idle":"2024-12-08T13:17:10.947939Z","shell.execute_reply.started":"2024-12-08T13:17:09.814816Z","shell.execute_reply":"2024-12-08T13:17:10.947232Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"if NEMO:\n    aligner = AlignerModel.from_pretrained(\"tts_en_radtts_aligner\")\n    text_normalizer = Normalizer(input_case=\"cased\", lang=\"en\")\n    arpabet_g2p = EnglishG2p(ignore_ambiguous_words=False)\n    arpabet_tokenizer = EnglishPhonemesTokenizer(arpabet_g2p)\n    \n    text = \"We are going to train mamba 1\"\n    normalized_text = text_normalizer.normalize(text)\n    arpabet_phonemes = arpabet_g2p(normalized_text)\n    arpabet_tokens = arpabet_tokenizer(normalized_text)\n    \n    print(text)\n    print(normalized_text)\n    print(arpabet_phonemes)\n    print(arpabet_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:17:10.948993Z","iopub.execute_input":"2024-12-08T13:17:10.949460Z","iopub.status.idle":"2024-12-08T13:17:10.954624Z","shell.execute_reply.started":"2024-12-08T13:17:10.949403Z","shell.execute_reply":"2024-12-08T13:17:10.953694Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"with open(ARPABET) as f:\n    lines = f.readlines()\n\nlines = [line.replace(\"\\n\", \"\").split(' ') for line in lines]\nchar_to_id = {c: int(i) for c, i in lines}\nEOS = max(char_to_id.values()) + 1\n\nnltk.download('averaged_perceptron_tagger_eng')\ng2p = G2p()\n\n\ndef frontend(text, debug=False):\n    text = custom_english_cleaners(text)\n    charseq = g2p(text)\n    idseq = []\n    for c in charseq:\n        if c.isspace():\n            idseq += [char_to_id[\"<space>\"]]\n        elif c not in char_to_id.keys():\n            idseq += [char_to_id[\"<unk>\"]]\n        else:\n            idseq += [char_to_id[c]]\n    # idseq += [EOS]\n\n    if debug:\n        print(text)\n        print(charseq)\n\n    return torch.LongTensor(idseq).view(-1)\n\n\ndef delete_punctuation(text):\n        normalized_text = text.translate(str.maketrans('', '', string.punctuation))\n        normalized_text = ' '.join(normalized_text.split())\n        return normalized_text\n\n\nfrontend('We are going to train TTS model with Mamba 1!!! )))', debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:17:10.955639Z","iopub.execute_input":"2024-12-08T13:17:10.955882Z","iopub.status.idle":"2024-12-08T13:17:12.251167Z","shell.execute_reply.started":"2024-12-08T13:17:10.955857Z","shell.execute_reply":"2024-12-08T13:17:12.247866Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\nWE ARE GOING TO TRAIN TTS MODEL WITH MAMBA ONE!!! \n['W', 'IY1', ' ', 'AA1', 'R', ' ', 'G', 'OW1', 'IH0', 'NG', ' ', 'T', 'UW1', ' ', 'T', 'R', 'EY1', 'N', ' ', 'T', 'IY1', 'EH1', 'N', 'IY1', 'S', ' ', 'M', 'AA1', 'D', 'AH0', 'L', ' ', 'W', 'IH1', 'DH', ' ', 'M', 'AA1', 'M', 'B', 'AH0', ' ', 'W', 'AH1', 'N', ' ', '!', ' ', '!', ' ', '!']\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"tensor([73, 46,  1,  9, 61,  1, 40, 55, 42, 53,  1, 64, 70,  1, 64, 61, 37, 52,\n         1, 64, 46, 31, 52, 46, 62,  1, 51,  9, 28, 14, 50,  1, 73, 43, 29,  1,\n        51,  9, 51, 26, 14,  1, 73, 15, 52,  1,  3,  1,  3,  1,  3])"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"encodec = EncodecModel.encodec_model_24khz()\nencodec.set_target_bandwidth(6.0)\n\ndef get_codecs(signal, sr):\n    signal = convert_audio(signal, sr, encodec.sample_rate, encodec.channels)\n    signal = signal.unsqueeze(0)\n    \n    with torch.no_grad():\n        encoded_frames = encodec.encode(signal)\n    codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # [B, n_q, T]\n    \n    return codes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:17:12.255709Z","iopub.execute_input":"2024-12-08T13:17:12.256674Z","iopub.status.idle":"2024-12-08T13:17:13.222409Z","shell.execute_reply.started":"2024-12-08T13:17:12.256610Z","shell.execute_reply":"2024-12-08T13:17:13.221650Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n  WeightNorm.apply(module, name, dim)\nDownloading: \"https://dl.fbaipublicfiles.com/encodec/v0/encodec_24khz-d7cc33bc.th\" to /root/.cache/torch/hub/checkpoints/encodec_24khz-d7cc33bc.th\n100%|██████████| 88.9M/88.9M [00:00<00:00, 194MB/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\")\n\ndef x_vector(signal):\n    embeddings = classifier.encode_batch(signal)\n    return embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:17:13.223387Z","iopub.execute_input":"2024-12-08T13:17:13.223686Z","iopub.status.idle":"2024-12-08T13:17:14.498856Z","shell.execute_reply.started":"2024-12-08T13:17:13.223658Z","shell.execute_reply":"2024-12-08T13:17:14.497917Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"hyperparams.yaml:   0%|          | 0.00/2.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f68419bded49495d85154499054c1ded"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"embedding_model.ckpt:   0%|          | 0.00/16.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"119c28ecbe21466eb1d1ea630b4a0ebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mean_var_norm_emb.ckpt:   0%|          | 0.00/3.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ecd87786c743a1b3b20f55862ed3bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"classifier.ckpt:   0%|          | 0.00/15.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"514b29d82d04462e83a0213ca765df0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a4cd27ff654e66ad050d229b3e5d73"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(path, map_location=device)\n/opt/conda/lib/python3.10/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  stats = torch.load(path, map_location=device)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class TtsDataset(Dataset):\n    def __init__(self, dir, sr=24000):\n        self.dir = dir\n        self.sr = sr\n        self.samples = self.load_samples()\n        self.ids = os.listdir(dir)\n\n    \n    def load_samples(self):\n        samples = {}\n        for root, _, files in os.walk(self.dir):\n            for file in files:\n                if file.endswith(\".wav\"):\n                    wav_path = os.path.join(root, file)\n                    transcript_path = wav_path.replace(\".wav\", \".normalized.txt\")\n                    if os.path.exists(transcript_path):\n                        speaker_id = os.path.basename(root)\n                        if speaker_id not in samples:\n                            samples[speaker_id] = []\n                        samples[speaker_id].append((wav_path, transcript_path))\n        \n        return samples\n\n\n    def get_data(self, audio_path, transcript_path):\n        audio_data, orig_sr = torchaudio.load(audio_path)\n\n        if orig_sr != self.sr:\n            audio_data = torchaudio.transforms.Resample(orig_sr, self.sr)(audio_data)\n\n        with open(transcript_path, \"r\") as f:\n            transcript = f.readline().strip()\n\n        return audio_data, transcript\n\n    \n    def listen(self, index):\n        waveform, transcript = self.get_data(index)\n        frontend(transcript, debug=True)\n        ipd.display(ipd.Audio(waveform, rate=self.sr))\n\n    \n    def plot_spec(self, index):\n        waveform, transcript = self.get_data(index)\n        frontend(transcript, debug=True)\n        ipd.display(ipd.Audio(waveform, rate=self.sr))\n        plt.figure(figsize=(15,5))\n        _ = plt.pcolormesh(melspectrogram(y=waveform, sr=self.sr), cmap='viridis')\n\n\n    def __len__(self):\n        return sum(len(files) for files in self.samples.values())\n\n    \n    def __getitem__(self, index):\n        flat_samples = [(speaker, pair) for speaker, pairs in self.samples.items() for pair in pairs]\n        \n        speaker, (audio_path, transcript_path) = flat_samples[index]\n        waveform, transcript = self.get_data(audio_path, transcript_path)\n        \n        speaker_samples = self.samples[speaker]\n        clone_audio_path, clone_transcript_path = random.choice(speaker_samples)\n        clone_waveform, _ = self.get_data(clone_audio_path, clone_transcript_path)\n\n        transcript = frontend(transcript)\n\n        data = get_codecs(torch.FloatTensor(waveform), sr=self.sr)[0]\n        data_c = get_codecs(torch.FloatTensor(clone_waveform), sr=self.sr)[0]\n\n        return transcript, data_c, data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:17:14.500330Z","iopub.execute_input":"2024-12-08T13:17:14.501084Z","iopub.status.idle":"2024-12-08T13:17:14.511384Z","shell.execute_reply.started":"2024-12-08T13:17:14.501054Z","shell.execute_reply":"2024-12-08T13:17:14.510591Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def collate_fn(batch):\n    # tensor batch with padding\n    \n    # max_transcript_len = max([len(item[0]) for item in batch])\n    # padded_transcripts = torch.stack([F.pad(item[0], (0, max_transcript_len - item[0].size(0))) for item in batch])\n\n    # max_len_data_c = max([item[1].size(1) for item in batch])\n    # padded_data_c = torch.stack([F.pad(item[1], (0, max_len_data_c - item[1].size(1))) for item in batch])\n    \n    # max_len_data = max([item[2].size(1) for item in batch])\n    # padded_data = torch.stack([F.pad(item[2], (0, max_len_data - item[2].size(1))) for item in batch])\n\n    # transcript_tensor = padded_transcripts.clone().detach()\n    # data_c_tensor = padded_data_c.clone().detach()\n    # data_tensor = padded_data.clone().detach()\n\n    \n    # list batch without padding\n\n    transcript_tensor = [item[0] for item in batch]\n    data_c_tensor = [item[1] for item in batch]\n    data_tensor = [item[2] for item in batch]\n\n    return transcript_tensor, data_c_tensor, data_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:17:14.514483Z","iopub.execute_input":"2024-12-08T13:17:14.514762Z","iopub.status.idle":"2024-12-08T13:17:14.527370Z","shell.execute_reply.started":"2024-12-08T13:17:14.514735Z","shell.execute_reply":"2024-12-08T13:17:14.526563Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def get_loaders(\n    sr = 24000,\n    batch_size=1,\n    num_workers=0,\n    pin_memory=True,\n):\n\n    train_ds = TtsDataset(\n        dir=TRAIN_DATA_PATH,\n        sr=sr\n    )\n\n    train_loader = DataLoader(\n        dataset=train_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        collate_fn=collate_fn,\n        shuffle=False\n    )\n\n    val_ds = TtsDataset(\n        dir=VAL_DATA_PATH,\n        sr=sr\n    )\n\n    val_loader = DataLoader(\n        dataset=val_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        collate_fn=collate_fn,\n        shuffle=False,\n    )\n\n    test_ds = TtsDataset(\n        dir=TEST_DATA_PATH,\n        sr=sr\n    )\n\n    test_loader = DataLoader(\n        dataset=test_ds,\n        batch_size=1,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        collate_fn=collate_fn,\n        shuffle=False,\n    )\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:17:14.528341Z","iopub.execute_input":"2024-12-08T13:17:14.528658Z","iopub.status.idle":"2024-12-08T13:17:14.536747Z","shell.execute_reply.started":"2024-12-08T13:17:14.528623Z","shell.execute_reply":"2024-12-08T13:17:14.535974Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_loader, val_loader, test_loader = get_loaders(batch_size=5, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:17:14.537730Z","iopub.execute_input":"2024-12-08T13:17:14.538006Z","iopub.status.idle":"2024-12-08T13:19:23.580256Z","shell.execute_reply.started":"2024-12-08T13:17:14.537971Z","shell.execute_reply":"2024-12-08T13:19:23.579498Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/xi-j/Mamba-ASR.git\n!cd Mamba-ASR\n\nimport sys\nsys.path.append('/kaggle/working/Mamba-ASR')\n\nfrom modules.mamba.bimamba import Mamba as BiMamba\n\nfrom mamba_ssm.ops.triton.layer_norm import RMSNorm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:23.581235Z","iopub.execute_input":"2024-12-08T13:19:23.581506Z","iopub.status.idle":"2024-12-08T13:19:26.092322Z","shell.execute_reply.started":"2024-12-08T13:19:23.581479Z","shell.execute_reply":"2024-12-08T13:19:26.091100Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Mamba-ASR'...\nremote: Enumerating objects: 95, done.\u001b[K\nremote: Counting objects: 100% (95/95), done.\u001b[K\nremote: Compressing objects: 100% (70/70), done.\u001b[K\nremote: Total 95 (delta 44), reused 61 (delta 23), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (95/95), 986.24 KiB | 9.22 MiB/s, done.\nResolving deltas: 100% (44/44), done.\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/Mamba-ASR/modules/mamba/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n/kaggle/working/Mamba-ASR/modules/mamba/selective_scan_interface.py:233: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout):\n/kaggle/working/Mamba-ASR/modules/mamba/selective_scan_interface.py:301: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n/kaggle/working/Mamba-ASR/modules/mamba/selective_scan_interface.py:374: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout):\n/kaggle/working/Mamba-ASR/modules/mamba/selective_scan_interface.py:446: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n/kaggle/working/Mamba-ASR/modules/mamba/selective_scan_interface.py:526: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, dout):\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def sinusoids(length, channels, max_timescale=10000):\n    assert channels % 2 == 0\n    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n\n\ndef list_to_tensors(tensors):\n    # Calculate lengths\n    l = list(map(len, tensors))\n\n    # Padded tensors\n    padded = pad_sequence(tensors, batch_first=True, padding_value=0)\n\n    # Mask\n    mask = torch.zeros((padded.shape[0], padded.shape[1]), device=padded.device)\n    for i in range(len(l)):\n        mask[i, l[i]:] = -10000.0\n    \n    return padded, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:26.094007Z","iopub.execute_input":"2024-12-08T13:19:26.094354Z","iopub.status.idle":"2024-12-08T13:19:26.101673Z","shell.execute_reply.started":"2024-12-08T13:19:26.094322Z","shell.execute_reply":"2024-12-08T13:19:26.100698Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, n_embed):\n        super().__init__()\n        self.ffn = nn.Sequential(\n            nn.Linear(n_embed, 4*n_embed),\n            nn.ReLU(),\n            nn.Linear(4*n_embed, n_embed),\n            nn.Dropout(0.2),\n        )\n    def forward(self, x):\n        return self.ffn(x)\n\n\nclass MambaEncoderBlock(nn.Module):\n    def __init__(self, n_embed, n_blocks):\n        super().__init__()\n\n        self.n_blocks = n_blocks\n      \n        self.blocks = nn.ModuleList([\n            BiMamba(\n                d_model=n_embed,  # Model dimension d_model\n                d_state=16,       # SSM state expansion factor\n                d_conv=4,         # Local convolution width\n                expand=4          # Block expansion factor\n            ).to(\"cuda\") for _ in range(n_blocks)\n        ])\n      \n        self.norms = nn.ModuleList([RMSNorm(n_embed) for _ in range(n_blocks)])\n        \n        self.ffn = FeedForward(n_embed)\n        self.ln1 = nn.LayerNorm(n_embed)\n        self.ln2 = nn.LayerNor(n_embed)\n\n\n    def forward(self, x):\n        resudial = x\n        for i in range(self.n_blocks):\n            res = x\n            x = self.norms[i](x)\n            x = self.blocks[i](x)\n            x = x + res\n            \n        x = res + self.ln1(x)\n        x = x + self.ffn(self.ln2(x))\n\n        return x\n\n\n\nclass MambaDecoderBlock(nn.Module):\n    def __init__(self, n_embed, n_blocks):\n        super().__init__()\n\n        self.n_blocks = n_blocks\n      \n        self.blocks = nn.ModuleList([\n            Mamba(\n                d_model=n_embed,  # Model dimension d_model\n                d_state=16,       # SSM state expansion factor\n                d_conv=4,         # Local convolution width\n                expand=4          # Block expansion factor\n            ).to(\"cuda\") for _ in range(n_blocks)\n        ])\n      \n        self.norms = nn.ModuleList([RMSNorm(n_embed) for _ in range(n_blocks)])\n        \n        self.ffn = FeedForward(n_embed)\n        self.ln1 = nn.LayerNorm(n_embed)\n        self.ln2 = nn.LayerNorm(n_embed)\n\n\n    def forward(self, x):\n        resudial = x\n        for i in range(self.n_blocks):\n            res = x\n            x = self.norms[i](x)\n            x = self.blocks[i](x)\n            x = x + res\n            \n        x = res + self.ln1(x)\n        x = x + self.ffn(self.ln2(x))\n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:26.102901Z","iopub.execute_input":"2024-12-08T13:19:26.103274Z","iopub.status.idle":"2024-12-08T13:19:26.118076Z","shell.execute_reply.started":"2024-12-08T13:19:26.103247Z","shell.execute_reply":"2024-12-08T13:19:26.117103Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class NAR(torch.nn.Module):\n    def __init__(self, n_enc, n_blocks):\n        super(NAR, self).__init__()\n        \n        self.n_embed = 128\n        self.max_seq_len = 8 * 128\n\n        self.n_enc = n_enc\n        self.n_blocks = n_blocks\n\n        self.encoders = nn.Sequential(*[MambaEncoderBlock(self.n_embed, n_blocks) for _ in range(n_enc)])\n\n        # Positional embeddings\n        # self.register_buffer(\"positional_embedding\", sinusoids(self.max_seq_len, self.n_embed))\n        self.positional_embedding_text = torch.nn.Embedding(self.max_seq_len, self.n_embed)\n        torch.nn.init.normal_(self.positional_embedding_text.weight, mean=0.0, std=0.02)\n        self.positional_embedding_audio = torch.nn.Embedding(self.max_seq_len, self.n_embed)\n        torch.nn.init.normal_(self.positional_embedding_audio.weight, mean=0.0, std=0.02)\n\n        # Text Condition\n        self.text_embedding = torch.nn.Embedding(80, self.n_embed)\n        torch.nn.init.normal_(self.text_embedding.weight, mean=0.0, std=0.02)\n\n        # Audio embedding\n        self.audio_embedding = torch.nn.Embedding(1024, self.n_embed)\n        torch.nn.init.normal_(self.audio_embedding.weight, mean=0.0, std=0.02)\n\n        # EOS embedding\n        self.eos_embedding = torch.nn.Embedding(1, self.n_embed)\n        torch.nn.init.normal_(self.eos_embedding.weight, mean=0.0, std=0.02)\n\n        # Codec index embedding\n        self.codec_index_embedding = torch.nn.Embedding(7, self.n_embed)\n        torch.nn.init.normal_(self.codec_index_embedding.weight, mean=0.0, std=0.02)\n\n        # Output prediction\n        # self.prediction = torch.nn.Linear(self.n_embed, 1024, bias=False)\n        self.prediction = torch.nn.Linear(self.n_embed, 1024)\n        torch.nn.init.normal_(self.prediction.weight, mean=0.0, std=0.02)\n        torch.nn.init.zeros_(self.prediction.bias)\n        \n    \n    def forward(self, *, condition_text, condition_audio, audio, codec, loss=False):\n\n        # Prepare\n        with record_function(\"prepare\"):\n            # Check inputs\n            device = condition_text[0].device\n            B = len(condition_text)\n            assert len(condition_audio) == B\n            assert len(audio) == B\n            assert len(codec) == B\n\n            # Check shapes\n            for b in range(B):\n\n                # Check condition shape\n                assert condition_text[b].dim() == 1, f\"Unexpected shape: {condition_text[b].shape}\"\n                assert condition_audio[b].dim() == 2, f\"Unexpected shape: {condition_audio[b].shape}\"\n                assert condition_audio[b].shape[0] == 8, f\"Unexpected shape: {condition_audio[b].shape}\"\n\n                # Check codec value\n                assert codec[b] >= 1 and codec[b] <= 7, f\"Unexpected codec value: {codec[b]}\"\n\n                # Check audio shape\n                assert audio[b].dim() == 2, f\"Unexpected shape: {audio[b].shape}\"\n                assert audio[b].shape[0] >= codec[b], f\"Unexpected shape: {audio[b].shape}\"\n\n            \n            # Prepare EOS\n            with record_function(\"prepare:eos\"):\n                eos = self.eos_embedding(torch.tensor([0]).to(device, non_blocking=True))\n        \n            \n            # Text embedding \n            with record_function(\"prepare:text\"):\n                l_t = []\n                x_t = []\n                for b in range(B):\n                    t = torch.cat([self.text_embedding(condition_text[b]), eos])\n                    # t = t + self.positional_embedding[:t.shape[0]]\n                    t = t + self.positional_embedding_text(torch.arange(t.shape[0]).to(t.device, non_blocking=True))\n                    x_t.append(t)\n                    l_t.append(t.shape[0])\n\n            \n            # Audio embedding\n            x_a = []\n            l_c = []\n            l_a = []\n            for b in range(B):\n\n                # Condition embedding\n                t_c = self.audio_embedding(condition_audio[b][0])\n                for i in range(1, condition_audio[b].shape[0]):\n                    t_c = t_c + self.audio_embedding(condition_audio[b][i])\n\n                # Audio embedding\n                t_a = self.audio_embedding(audio[b][0])\n                for i in range(1, codec[b]):\n                    t_a = t_a + self.audio_embedding(audio[b][i])\n            \n                # Concatenate all\n                t = torch.cat([t_c, t_a, eos])\n\n                # Positional embedding\n                # t = t + self.positional_embedding[:t.shape[0]]\n                t = t + self.positional_embedding_audio(torch.arange(t.shape[0]).to(t.device, non_blocking=True))\n\n                # Appends\n                x_a.append(t)\n                l_c.append(t_c.shape[0])\n                l_a.append(t_a.shape[0])\n\n            # Codec embedding\n            x_ci = []\n            for b in range(B):\n                t_ci = self.codec_index_embedding(torch.tensor([codec[b] - 1]).long().to(t.device, non_blocking=True))\n                x_ci.append(t_ci)\n        \n            # Concatenate all\n            x = []\n            for b in range(B):\n                x.append(torch.cat([x_t[b], x_a[b], x_ci[b]]))\n            x, m = list_to_tensors(x)\n\n        # Transform\n        x = self.encoders(x)\n\n        # Predict\n        x = self.prediction(x)\n\n        # Extract predictions\n        predicted = []\n        for i in range(B):\n            p_s = l_t[i] + l_c[i]\n            p_e = p_s + l_a[i]\n            predicted.append(x[i, p_s:p_e])\n\n        # Loss\n        if loss:\n            source = pad_sequence(predicted, batch_first=True, padding_value=-1)\n            targets = pad_sequence([audio[i][codec[i]] for i in range(B)], batch_first=True, padding_value=-1)\n            loss = F.cross_entropy(source.view(-1, source.size(-1)), targets.view(-1), ignore_index = -1)\n            return predicted, loss\n        else:\n            return predicted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:26.119376Z","iopub.execute_input":"2024-12-08T13:19:26.119697Z","iopub.status.idle":"2024-12-08T13:19:26.139084Z","shell.execute_reply.started":"2024-12-08T13:19:26.119670Z","shell.execute_reply":"2024-12-08T13:19:26.138158Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class AR(torch.nn.Module):\n    def __init__(self, n_enc, n_blocks):\n        super(AR, self).__init__()\n        \n        self.n_embed = 128\n        self.max_seq_len = 8 * 128\n\n        self.n_enc = n_enc\n        self.n_blocks = n_blocks\n\n        self.decoders = nn.Sequential(*[MambaDecoderBlock(self.n_embed, n_blocks) for _ in range(n_enc)])\n\n        # Positional embeddings\n        self.positional_embedding_text = torch.nn.Embedding(self.max_seq_len, self.n_embed)\n        torch.nn.init.normal_(self.positional_embedding_text.weight, mean=0.0, std=0.02)\n        self.positional_embedding_audio = torch.nn.Embedding(self.max_seq_len, self.n_embed)\n        torch.nn.init.normal_(self.positional_embedding_audio.weight, mean=0.0, std=0.02)\n\n        # Text embeddings\n        self.text_embedding = torch.nn.Embedding(80, self.n_embed)\n        torch.nn.init.normal_(self.text_embedding.weight, mean=0.0, std=0.02)\n\n        # Audio embeddings\n        self.audio_embedding = torch.nn.Embedding(1024, self.n_embed)\n        torch.nn.init.normal_(self.audio_embedding.weight, mean=0.0, std=0.02)\n\n        # EOS embedding\n        self.eos_embedding = torch.nn.Embedding(1, self.n_embed)\n        torch.nn.init.normal_(self.eos_embedding.weight, mean=0.0, std=0.02)\n\n        # BOS embedding\n        self.bos_embedding = torch.nn.Embedding(1, self.n_embed)\n        torch.nn.init.normal_(self.bos_embedding.weight, mean=0.0, std=0.02)\n\n        # Output prediction\n        self.prediction = torch.nn.Linear(self.n_embed, 1025)\n        torch.nn.init.normal_(self.prediction.weight, mean=0.0, std=0.02)\n        torch.nn.init.zeros_(self.prediction.bias)\n\n    def forward(self, *, text, audio, loss=False):\n        \n        # Check shapes\n        device = text[0].device\n        B = len(text)\n        assert len(text) == B\n        assert len(audio) == B\n        for b in range(B):\n            assert text[b].dim() == 1, f\"Unexpected shape: {text[b].shape}\"\n            assert audio[b].dim() == 2, f\"Unexpected shape: {audio[b].shape}\"\n\n        # Prepare EOS/BOS\n        eos = self.eos_embedding(torch.tensor([0]).to(device, non_blocking=True))\n        bos = self.bos_embedding(torch.tensor([0]).to(device, non_blocking=True))\n\n        # Text embedding\n        l_t = []\n        x_t = []\n        for b in range(B):\n\n            # Text\n            t = torch.cat([self.text_embedding(text[b]), eos])\n\n            # Positional embedding\n            t = t + self.positional_embedding_text(torch.arange(t.shape[0]).to(t.device, non_blocking=True))\n\n            # Append\n            x_t.append(t)\n            l_t.append(t.shape[0])\n\n        # Audio embedding\n        x_a = []\n        l_a = []\n        for b in range(B):\n\n            # Audio\n            t = torch.cat([bos, self.audio_embedding(audio[b][0])])\n\n            # Positional embedding\n            t = t + self.positional_embedding_audio(torch.arange(t.shape[0]).to(t.device, non_blocking=True))\n\n            # Append\n            x_a.append(t)\n            l_a.append(t.shape[0])\n\n        # Concatenate\n        x = []\n        for b in range(B):\n            x.append(torch.cat([x_t[b], x_a[b]]))\n        x, m = list_to_tensors(x)\n\n        # Transform\n        x = self.decoders(x)\n\n        # Predict\n        x = self.prediction(x)\n\n        # Extract predictions\n        predicted = []\n        for i in range(B):\n            p_s = l_t[i]\n            p_e = p_s + l_a[i]\n            predicted.append(x[i, p_s:p_e])\n\n        # Loss\n        if loss:\n            source = pad_sequence(predicted, batch_first=True, padding_value=-1)\n            targets = pad_sequence([torch.cat([audio[i][0], torch.tensor([1024], dtype = torch.int).to(t.device, non_blocking=True)]) for i in range(B)], batch_first=True, padding_value=-1)\n            loss = F.cross_entropy(source.view(-1, source.size(-1)), targets.view(-1), ignore_index = -1)\n            return predicted, loss\n        else:\n            return predicted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:26.140404Z","iopub.execute_input":"2024-12-08T13:19:26.140882Z","iopub.status.idle":"2024-12-08T13:19:26.155985Z","shell.execute_reply.started":"2024-12-08T13:19:26.140845Z","shell.execute_reply":"2024-12-08T13:19:26.155208Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"m = AR(2, 2).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:26.156844Z","iopub.execute_input":"2024-12-08T13:19:26.157083Z","iopub.status.idle":"2024-12-08T13:19:26.198245Z","shell.execute_reply.started":"2024-12-08T13:19:26.157059Z","shell.execute_reply":"2024-12-08T13:19:26.197601Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"t, a_c, a = next(iter(train_loader))\nfor i in range(len(t)):\n    t[i] = t[i].to(device)\n    a_c[i] = a_c[i].to(device)\n    a[i] = a[i].to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:26.199190Z","iopub.execute_input":"2024-12-08T13:19:26.199532Z","iopub.status.idle":"2024-12-08T13:19:39.767388Z","shell.execute_reply.started":"2024-12-08T13:19:26.199494Z","shell.execute_reply":"2024-12-08T13:19:39.765869Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"a[1].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:21:18.777902Z","iopub.execute_input":"2024-12-08T13:21:18.778658Z","iopub.status.idle":"2024-12-08T13:21:18.784004Z","shell.execute_reply.started":"2024-12-08T13:21:18.778621Z","shell.execute_reply":"2024-12-08T13:21:18.783072Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"torch.Size([8, 141])"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"m(text=t, audio=a)[1].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:21:20.677692Z","iopub.execute_input":"2024-12-08T13:21:20.678696Z","iopub.status.idle":"2024-12-08T13:21:20.696009Z","shell.execute_reply.started":"2024-12-08T13:21:20.678646Z","shell.execute_reply":"2024-12-08T13:21:20.695211Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"torch.Size([142, 1025])"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"# Train AR","metadata":{}},{"cell_type":"code","source":"assert False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:43.515602Z","iopub.execute_input":"2024-12-08T13:19:43.515858Z","iopub.status.idle":"2024-12-08T13:19:43.704806Z","shell.execute_reply.started":"2024-12-08T13:19:43.515833Z","shell.execute_reply":"2024-12-08T13:19:43.700159Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mAssertionError\u001b[0m: "],"ename":"AssertionError","evalue":"","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"wandb.init(\n      project=\"Mamba-TTS\",\n      name=f\"experiment_{'AR'}\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:43.705582Z","iopub.status.idle":"2024-12-08T13:19:43.705893Z","shell.execute_reply.started":"2024-12-08T13:19:43.705748Z","shell.execute_reply":"2024-12-08T13:19:43.705763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_experiment = \"valle-ar\"\ntrain_project=\"mamba-valle-ar\"\ntrain_auto_resume = True\ntrain_batch_size = 8\ntrain_lr_start = 1e-12\ntrain_lr_max = 5e-4\ntrain_steps = 10\ntrain_warmup_steps = 32000 \ntrain_mixed_precision = \"fp16\"  # \"fp16\" or None\ntrain_clip_grad_norm = 1\n\ntrain_loader_workers = 4\ntrain_log_every = 10\ntrain_save_every = 10\ntrain_watch_every = 1000\ntrain_evaluate_every = 1\n\n# initialization\naccelerator = Accelerator(mixed_precision=train_mixed_precision)\noutput_dir = Path(\"/kaggle/working\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# data\n# accelerator.print(\"Loading dataset...\")\n# tokenizer = Tokenizer(\"./tokenizer_text.model\")\n# train_sampler = load_sampler(\"./external_datasets/libriheavy/libriheavy_cuts_large.jsonl.gz\", \n#                              \"./external_datasets/libriheavy-large-encodec/\", train_batch_size, tokenizer)\n# train_loader = create_async_loader(train_sampler, num_workers=train_loader_workers)\n\n# model\naccelerator.print(\"Loading model...\")\nmodel = AR(2, 3).to(device)\noptim = AdamW(model.parameters(), lr=train_lr_start, betas=(0.9, 0.95), weight_decay=0.01, eps=1e-6)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=train_steps)\n\n# wandb\nwandb.init(project=train_project, name=train_experiment)\n\n\n# save model function\ndef save():\n    checkpoint_path = output_dir / f\"{train_experiment}.pt\"\n    torch.save({\n        'model': model.state_dict(),\n        'optimizer': optim.state_dict(),\n        'scheduler': scheduler.state_dict(),\n        'step': step,\n    }, checkpoint_path)\n    wandb.save(str(checkpoint_path))\n\n\n# train function\ndef train_step():\n    train_loss = 0.0\n    \n    model.train()\n    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n        text, сondition, audio = batch\n        # text = text.to(device)\n        # audio = audio.to(device)   # we use list batch\n\n        max_len = max(len(aud) for aud in audio)\n        batch_size = len(audio)\n\n        for t in range(max_len):\n            outputs, loss = model(text=text, audio=audio, loss=True)\n            for output in outputs:\n                \n        \n        optim.zero_grad()\n        \n        # forward\n        outputs, loss = model(text=text, audio=audio, loss=True)\n        accelerator.backward(loss)\n        if accelerator.sync_gradients:\n            accelerator.clip_grad_norm_(model.parameters(), train_clip_grad_norm)\n        optim.step()\n\n        train_loss += loss.item()\n        \n    train_loss = train_loss / len(train_loader)\n\n    accelerator.print(f\"Train: Loss = {train_loss:.4f}\")\n    return train_loss\n\n\n# val function\ndef validate():\n    val_loss = 0.0\n\n    model.eval()\n    with torch.no_grad():\n        for batch in val_loader:\n            text, dictor, audio = batch\n            outputs, loss = model(text=text, audio=audio, loss=True)\n            val_loss += loss.item()\n\n    val_loss = val_loss / len(val_loader)\n\n    accelerator.print(f\"Validation: Loss = {val_loss:.4f}\")\n    return val_loss\n\n\n# train cycle\nstep = 1\nwhile step <= train_steps:\n    start_time = time.time()\n\n    # train step\n    loss = train_step()\n    \n    # validation\n    if step % train_evaluate_every == 0:\n        val_loss = validate()\n        \n    # log\n    if step % train_log_every == 0:\n        wandb.log({\n            \"step\": step,\n            \"train_loss\": val_loss,\n            \"val_loss\": val_loss,\n            \"learning_rate\": scheduler.get_last_lr()[0],\n        })\n\n    # save\n    if step % train_save_every == 0:\n        save()\n\n    # step & time update\n    step += 1\n    end_time = time.time()\n    accelerator.print(f\"Step {step} completed in {end_time - start_time:.2f} seconds\")\n\n    # scheduler -> lr\n    scheduler.step()\n\n# the end\naccelerator.print(\"Training completed.\")\nsave()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:43.707395Z","iopub.status.idle":"2024-12-08T13:19:43.707884Z","shell.execute_reply.started":"2024-12-08T13:19:43.707642Z","shell.execute_reply":"2024-12-08T13:19:43.707668Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Draft","metadata":{}},{"cell_type":"code","source":"#hyperparams\n\nepochs = 20\nlr = 1e-3\nblock_size = 2000\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmax_iters = 10000\nprint_iters = 100\neval_iters = 10\neval_interval = 300\nn_embed=384\nn_heads = 6\nn_layers = 6\ndropout = 0.2\nvocab_size=1024\nfrom tqdm import tqdm\n\n# ---------\n\n\n\n\nclass SelfAttentionHead(nn.Module):\n  def __init__(self, head_size):\n    super().__init__()\n    self.keys = nn.Linear(n_embed, head_size)\n    self.queries = nn.Linear(n_embed, head_size)\n    self.values = nn.Linear(n_embed, head_size)\n    self.head_size = head_size\n    self.n_embed = n_embed\n    self.register_buffer('tril', torch.tril(torch.ones((block_size,block_size))).to(device))\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    B,T,C = x.shape\n    k = self.keys(x) # (B,T,C_h)\n    q = self.queries(x) # (B,T,C_h)\n    v = self.values(x) # (B,T,C_h)\n    wei = k @ q.transpose(-1,-2) * C**(-0.5)# (B,T,T)\n    wei = wei.masked_fill( self.tril[:T,:T]==0, float('-inf'))\n    # wei = F.softmax(wei, dim=-1) # (B,T,T)\n    wei = torch.log(torch.exp(wei)+1) # (B,T,T)\n    wei = self.dropout(wei)\n    out = wei @ v # (B,T,C_h)\n    return out\n\n\nclass MultiHeadAttention(nn.Module):\n  def __init__(self, n_heads, head_size) -> None:\n    super().__init__()\n    self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(n_heads)])\n    self.proj = nn.Linear(n_embed, n_embed)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    B,T,C = x.shape\n    out = torch.cat([head(x) for head in self.heads], dim=-1)\n    out = self.proj(out)\n    out = self.dropout(out)\n    return out\n\nclass FeedForward(nn.Module):\n  def __init__(self, n_embed) -> None:\n    super().__init__()\n    self.ffn = nn.Sequential(\n      nn.Linear(n_embed, 4*n_embed),\n      nn.ReLU(),\n      nn.Linear(4*n_embed, n_embed),\n      nn.Dropout(dropout),\n    )\n  def forward(self, x):\n    return self.ffn(x)\n\nclass Block(nn.Module):\n  def __init__(self, n_embed, n_heads) -> None:\n    super().__init__( )\n    self.head_size = n_embed // n_heads\n    # self.sa_head = MultiHeadAttention(n_heads, self.head_size)\n    self.sa_head = Mamba(\n      # This module uses roughly 3 * expand * d_model^2 parameters\n      d_model=n_embed, # Model dimension d_model\n      d_state=16,  # SSM state expansion factor\n      d_conv=4,    # Local convolution width\n      expand=1,    # Block expansion factor\n  ).to(\"cuda\")\n    self.ffn = FeedForward(n_embed)\n    self.ln1 = nn.LayerNorm(n_embed)\n    self.ln2 = nn.LayerNorm(n_embed)\n\n\n  def forward(self, x):\n    x = x + self.sa_head(self.ln1(x))\n    x = x + self.ffn(self.ln2(x))\n\n    return x\n\nclass MambaAudioModel(nn.Module):\n  def __init__(self,vocab_size):\n    super().__init__()\n    self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n    self.position_embedding_table = nn.Embedding(block_size,n_embed)\n    self.lm_head = nn.Linear(n_embed,vocab_size)\n    self.ffn = FeedForward(n_embed)\n    print(\"layers\", n_layers)\n    self.blocks = nn.Sequential(*[Block(n_embed,n_heads=n_heads) for _ in range(n_layers)])\n\n\n  def forward(self, idx, targets=None):\n    # idx = idx[:,-block_size:]\n    B,T = idx.shape\n    tok_emb = self.token_embedding_table(idx) # (B,T, C_e)\n    pos_emb = self.position_embedding_table(torch.arange(T,device=device)) # (T, C_e)\n    x = tok_emb + pos_emb # (B,T,Q, C_e)\n    x = self.blocks(x) # (B,T,Q, C_e)\n    logits = self.lm_head(x) # (B,T,vocab_size)\n    if targets is None:\n      loss = None\n    else:\n      B,T,C = logits.shape\n      logits = logits.view(B*T,C)\n      targets = targets.view(B*T)\n      loss = F.cross_entropy(logits, targets)\n      logits = logits.view(B,T,C)\n    return logits, loss\n\n\ndef estimate_test_loss(model, dataset):\n  model.eval()\n  test_losses = []\n  with torch.no_grad():\n    for tokens in tqdm(dataset['test']['tokens']):\n      tokens = tokens.to(device)\n      x = tokens[:,:-1].contiguous()\n      y = tokens[:,1:].contiguous()\n      logits, loss = model(x,y )\n      test_losses.append(loss)\n  model.train()\n  return sum(test_losses)/len(test_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:43.710010Z","iopub.status.idle":"2024-12-08T13:19:43.710348Z","shell.execute_reply.started":"2024-12-08T13:19:43.710191Z","shell.execute_reply":"2024-12-08T13:19:43.710209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, Audio\nfrom transformers import EncodecModel, AutoProcessor\n\n# dummy dataset, however you can swap this with an dataset on the 🤗 hub or bring your own\nlibrispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n# load the model + processor (for pre-processing the audio)\nmodel = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\nprocessor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n\n# cast the audio data to the correct sampling rate for the model\nlibrispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\naudio_sample = librispeech_dummy[0][\"audio\"][\"array\"]\n\n# pre-process the inputs\ninputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n\n# explicitly encode then decode the audio inputs\nencoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\naudio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n\n# or the equivalent with a forward pass\naudio_values = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values\n\n# you can also extract the discrete codebook representation for LM tasks\n# output: concatenated tensor of all the representations\naudio_codes = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_codes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:43.711754Z","iopub.status.idle":"2024-12-08T13:19:43.712043Z","shell.execute_reply.started":"2024-12-08T13:19:43.711903Z","shell.execute_reply":"2024-12-08T13:19:43.711919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ipd.Audio(audio_values.detach().numpy()[0], rate=22050)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:43.713498Z","iopub.status.idle":"2024-12-08T13:19:43.713835Z","shell.execute_reply.started":"2024-12-08T13:19:43.713680Z","shell.execute_reply":"2024-12-08T13:19:43.713698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_outputs.audio_codes.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:43.715687Z","iopub.status.idle":"2024-12-08T13:19:43.716012Z","shell.execute_reply.started":"2024-12-08T13:19:43.715860Z","shell.execute_reply":"2024-12-08T13:19:43.715877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = torch.randint(0, 1024, (1, 1, 2, 300))\na = model.decode(x, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:43.716950Z","iopub.status.idle":"2024-12-08T13:19:43.717295Z","shell.execute_reply.started":"2024-12-08T13:19:43.717108Z","shell.execute_reply":"2024-12-08T13:19:43.717125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ipd.Audio(a.detach().numpy()[0], rate=22050)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:19:43.718330Z","iopub.status.idle":"2024-12-08T13:19:43.718678Z","shell.execute_reply.started":"2024-12-08T13:19:43.718521Z","shell.execute_reply":"2024-12-08T13:19:43.718543Z"}},"outputs":[],"execution_count":null}]}