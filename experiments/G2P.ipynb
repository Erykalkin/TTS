{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\George.LAPTOP-TLP259VH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'HH EH1 L OW0'},\n",
       " {'generated_text': 'EH1 N V IH1 D IY0 AH0'},\n",
       " {'generated_text': 'IH1 Z'},\n",
       " {'generated_text': 'N AA1 T'},\n",
       " {'generated_text': 'R IH0 P AA1 Z AH0 T AO2 R IY0'},\n",
       " {'generated_text': 'P R AH0 V AY1 D Z'},\n",
       " {'generated_text': 'AO1 L'},\n",
       " {'generated_text': 'DH'},\n",
       " {'generated_text': 'N EH1 S AH0 S EH2 R IY0'},\n",
       " {'generated_text': 'T UW1 L Z'},\n",
       " {'generated_text': 'T UW1'},\n",
       " {'generated_text': 'P ER0 F AO1 R M'},\n",
       " {'generated_text': 'IH1 NG G AH0 L IH0 SH'},\n",
       " {'generated_text': 'G R EY1 F M T UW0 F OW2 N M'},\n",
       " {'generated_text': 'K AH0 N V ER1 ZH AH0 N'},\n",
       " {'generated_text': 'W IH1 TH'},\n",
       " {'generated_text': 'EY1'},\n",
       " {'generated_text': 'P R IY0 T R EY1 N D'},\n",
       " {'generated_text': 'S AW1 N D CH OY2 S'},\n",
       " {'generated_text': 'JH IY1 S IY2 P IY1'},\n",
       " {'generated_text': 'M AA1 D AH0 L'},\n",
       " {'generated_text': 'Y UW1 Z IH0 NG'},\n",
       " {'generated_text': 'S P IY1 CH B R EY2 N'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(task=\"text2text-generation\", model=\"cisco-ai/mini-bart-g2p\")\n",
    "text = \"Hello NVIDIA is not repository provides all the necessary tools to perform English grapheme-to-phoneme conversion with a pretrained SoundChoice G2P model using SpeechBrain.\"\n",
    "\n",
    "pipe(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\George.LAPTOP-TLP259VH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.inference.text import GraphemeToPhoneme\n",
    "g2p = GraphemeToPhoneme.from_hparams(\"speechbrain/soundchoice-g2p\", savedir=\"pretrained_models/soundchoice-g2p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DH', 'AE', 'T', ' ', 'IH', 'Z', ' ', 'N', 'AA', 'T', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "text = \"That is not\"\n",
    "phonemes = g2p(text)\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HH', 'EH', 'L', 'OW', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "phonemes = g2p('hello')\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'charsiu/g2p_multilingual_byT5_small_100'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'charsiu/g2p_multilingual_byT5_small_100' is the correct path to a directory containing all relevant files for a ByT5Tokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\George.LAPTOP-TLP259VH\\Documents\\GitHub\\TTS\\experiments\\G2P.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/George.LAPTOP-TLP259VH/Documents/GitHub/TTS/experiments/G2P.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipe2 \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtext2text-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcharsiu/g2p_multilingual_byT5_small_100\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\George.LAPTOP-TLP259VH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:999\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m             tokenizer_kwargs \u001b[39m=\u001b[39m model_kwargs\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    997\u001b[0m             tokenizer_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 999\u001b[0m         tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m   1000\u001b[0m             tokenizer_identifier, use_fast\u001b[39m=\u001b[39;49muse_fast, _from_pipeline\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenizer_kwargs\n\u001b[0;32m   1001\u001b[0m         )\n\u001b[0;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m load_image_processor:\n\u001b[0;32m   1004\u001b[0m     \u001b[39m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m     \u001b[39mif\u001b[39;00m image_processor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\George.LAPTOP-TLP259VH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:907\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    905\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTokenizer class \u001b[39m\u001b[39m{\u001b[39;00mtokenizer_class_candidate\u001b[39m}\u001b[39;00m\u001b[39m does not exist or is not currently imported.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m         )\n\u001b[1;32m--> 907\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    909\u001b[0m \u001b[39m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[39m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mc:\\Users\\George.LAPTOP-TLP259VH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2200\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2197\u001b[0m \u001b[39m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m \u001b[39m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2199\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2200\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2201\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2203\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2204\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2205\u001b[0m     )\n\u001b[0;32m   2207\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   2208\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'charsiu/g2p_multilingual_byT5_small_100'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'charsiu/g2p_multilingual_byT5_small_100' is the correct path to a directory containing all relevant files for a ByT5Tokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "pipe2 = pipeline(\"text2text-generation\", model=\"charsiu/g2p_multilingual_byT5_small_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words as n2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidOperation",
     "evalue": "[<class 'decimal.ConversionSyntax'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidOperation\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\George.LAPTOP-TLP259VH\\Documents\\GitHub\\TTS\\experiments\\G2P.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/George.LAPTOP-TLP259VH/Documents/GitHub/TTS/experiments/G2P.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m n2w(\u001b[39m'\u001b[39;49m\u001b[39m198-412\u001b[39;49m\u001b[39m'\u001b[39;49m, to\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcardinal\u001b[39;49m\u001b[39m'\u001b[39;49m, lang\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\George.LAPTOP-TLP259VH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\num2words\\__init__.py:95\u001b[0m, in \u001b[0;36mnum2words\u001b[1;34m(number, ordinal, lang, to, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m converter \u001b[39m=\u001b[39m CONVERTER_CLASSES[lang]\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(number, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 95\u001b[0m     number \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mstr_to_number(number)\n\u001b[0;32m     97\u001b[0m \u001b[39m# backwards compatible\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m ordinal:\n",
      "File \u001b[1;32mc:\\Users\\George.LAPTOP-TLP259VH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\num2words\\base.py:101\u001b[0m, in \u001b[0;36mNum2Word_Base.str_to_number\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstr_to_number\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m Decimal(value)\n",
      "\u001b[1;31mInvalidOperation\u001b[0m: [<class 'decimal.ConversionSyntax'>]"
     ]
    }
   ],
   "source": [
    "n2w('198412', to='cardinal', lang='en')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
